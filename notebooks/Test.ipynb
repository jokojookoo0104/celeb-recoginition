{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from triplet import *\n",
    "from data import *\n",
    "from preprocess import *\n",
    "from model import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, Activation, Input, Conv2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = './face'\n",
    "name = 'Tran Thanh'\n",
    "reader = LFWReader(dir_images=root,data_name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_input(batch_image):\n",
    "    if not isinstance(batch_image, (np.ndarray, np.generic)):\n",
    "        error_msg = \"data must be 4d numpy array, but found {}\"\n",
    "        raise TypeError(error_msg.format(type(batch_image)))\n",
    "    shape = batch_image.shape\n",
    "    if len(shape) != 4:\n",
    "        error_msg = \"data must be shape of (batch, 224, 224, 3), but found {}\"\n",
    "        raise ValueError(error_msg.format(shape))\n",
    "    (batch, size0, size1, channel) = shape\n",
    "    if size0 != 224 or size1 != 224 or channel != 3:\n",
    "        error_msg = \"data must be shape of (batch, 224, 224, 3), but found {}\"\n",
    "        raise ValueError(error_msg.format(shape))\n",
    "        \n",
    "    batch_image = batch_image.astype(np.float32)\n",
    "    batch_image = batch_image.transpose([0,3,1,2])\n",
    "    batch_image = batch_image.transpose([0,2,3,1])\n",
    "    return batch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _ShowImg(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def _ReadAndResize(filepath):\n",
    "    im = Image.open((filepath)).convert('RGB')\n",
    "    im = im.resize((224, 224))\n",
    "    return np.array(im, dtype=\"float32\")\n",
    "\n",
    "def _Flip(im_array):\n",
    "    if np.random.uniform(0, 1) > 0.7:\n",
    "        im_array = np.fliplr(im_array)\n",
    "    return im_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "list_pos = []\n",
    "list_anchor = []\n",
    "list_neg = []\n",
    "for _ in range(batch_size):\n",
    "    path_anchor, path_pos, path_neg = reader.GetTripletSingleID()\n",
    "    img_anchor = _Flip(_ReadAndResize(path_anchor))\n",
    "    img_pos = _Flip(_ReadAndResize(path_pos))\n",
    "    img_neg = _Flip(_ReadAndResize(path_neg))\n",
    "    list_pos.append(img_pos)\n",
    "    list_anchor.append(img_anchor)\n",
    "    list_neg.append(img_neg)\n",
    "    print((np.array(img_pos)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader.GetTripletSingleID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = TripletGeneratorSingleID(reader)\n",
    "data=next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _TestTripletGenerator(reader):  \n",
    "    gen = TripletGeneratorSingleID(reader)\n",
    "    data = next(gen)\n",
    "    imgs_anchor = data[0]['anchor_input']\n",
    "    imgs_pos = data[0]['positive_input']\n",
    "    imgs_neg = data[0]['negative_input']\n",
    "    print(imgs_anchor.shape)\n",
    "    print(imgs_pos.shape)\n",
    "    print(imgs_neg.shape)\n",
    "    #imgs_anchor = imgs_anchor.transpose([0,2,3,1])\n",
    "    #imgs_pos = imgs_pos.transpose([0,2,3,1])\n",
    "    #imgs_neg = imgs_neg.transpose([0,2,3,1])\n",
    "    \n",
    "    for idx_img in range(batch_size):\n",
    "        anchor = imgs_anchor[idx_img]\n",
    "        pos = imgs_pos[idx_img]\n",
    "        neg = imgs_neg[idx_img]\n",
    "        print(anchor.shape)\n",
    "        print(pos.shape)\n",
    "        print(neg.shape)\n",
    "        _ShowImg(anchor)\n",
    "        _ShowImg(pos)\n",
    "        _ShowImg(neg)\n",
    "        break\n",
    "    \n",
    "    print('data size is {}'.format(reader.GetDataSize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_TestTripletGenerator(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_train = TripletGeneratorSingleID(reader)\n",
    "gen_test = TripletGeneratorSingleID(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_model, triplet_model = GetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in embedding_model.layers[-3:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for layer in embedding_model.layers[: -3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "triplet_model.compile(loss=None, optimizer=Adam(0.001))\n",
    "\n",
    "history = triplet_model.fit_generator(gen_train, \n",
    "                          validation_data=gen_test,  \n",
    "                          epochs=1, \n",
    "                          verbose=1, \n",
    "                            steps_per_epoch=50,\n",
    "                          validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_model.save_weights('/home/sbikevn360/celebrities-recognition/weights/weights_finetune_50_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VggFace(\n",
    "    path = '/home/sbikevn360/celebrities-recognition/weights/weights_finetune_50_5.h5',\n",
    "                              is_origin = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/sbikevn360/celebrities-recognition/weights/weights_finetune_50_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('/home/sbikevn360/celebrities-recognition/weights/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = load_metadata('/home/sbikevn360/celebrities-recognition/face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    " \n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedded = np.zeros((metadata.shape[0], 2622 ))\n",
    "for i in range(metadata.shape[0]):\n",
    "    img_emb = model.predict(preprocess_image(metadata[i].image_path()))[0,:]\n",
    "    embedded[i] = img_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2h41\n",
    "# save embedding\n",
    "np.savetxt('/home/sbikevn360/celebrities-recognition/embedding/embedded_vector.txt', embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = []\n",
    "for i in range(len(metadata)):\n",
    "    name.append(metadata[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('/home/sbikevn360/celebrities-recognition/embedding/name.txt', name,  delimiter=\" \", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('/home/sbikevn360/celebrities-recognition/embedding/embedded_vector.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.jpg',\n",
       " '10.jpg',\n",
       " '8.jpg',\n",
       " '13.jpg',\n",
       " '14.jpg',\n",
       " '4.jpg',\n",
       " '6.jpg',\n",
       " '1.jpg',\n",
       " '16.jpg',\n",
       " '3.jpg',\n",
       " '9.jpg']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./face/Tran Thanh/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm -rf /face/Tran Thanh/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm -rf /face/Tran Thanh/.ipynb_checkpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
